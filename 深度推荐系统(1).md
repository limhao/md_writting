---
title: 深度推荐系统 上
---

# 1. 深度学习推荐模型（1）

演化方式：

1. 改变神经网络的复杂程度

2. 改变特征交叉方式

3. wide&deep模型

4. FM深度版本

5. 注意力机制与推荐系统结合

6. 序列模型与推荐模型结合

7. 强化学习与深度学习结合
## 2.1 神经网络复杂程度
### 2.1.1 Auto-rec--单层神经网络推荐模型（easy）

通过自编码器原理，还原输入的结果。

重建函数：   ![](https://www.zhihu.com/equation?tex=h%5Cleft%28+%5Cmathbf%7Br%7D%3B%5Ctheta+%5Cright%29%3Df%5Cleft%28+%5Cmathbf%7BW%7D+%5Ccdot+g%5Cleft%28+%5Cmathbf%7BVr%7D%2B%5Cmu+%5Cright%29+%2Bb+%5Cright%29) 



目标函数：  ![](https://www.zhihu.com/equation?tex=%5Cmathop%7Bmin%7D%5Climits_%7B%5Ctheta%7D+%5Csum_%7B%5Cmathbf%7Br%7D+%5Cin+%5Cmathbf%7BS%7D%7D%7B+%5ClVert+%5Cmathbb%7Br%7D-h%5Cleft%28+%5Cmathbf%7Br%7D%3B%5Ctheta+%5Cright%29+%5CrVert%7D_2%5E2) 

目标函数l2正则： ![](https://www.zhihu.com/equation?tex=%5Cmathop%7Bmin%7D%5Climits_%7B%5Ctheta%7D+%5Csum_%7Bi%3D1%7D%5En%7B+%5ClVert+%5Cmathbb%7Br%7D%5E%7B%28i%29%7D-h%5Cleft%28+%5Cmathbf%7Br%7D%5E%7B%28i%29%7D%3B%5Ctheta+%5Cright%29%5CrVert%7D_%7B%5Cmathcal%7BO%7D%7D%5E2+%2B%5Cfrac%7B%5Clambda%7D%7B2%7D%5Ccdot+%5Cleft%28+%5ClVert+%5Cmathbf%7BW%7D%5CrVert_F%5E2+%2B%5ClVert+%5Cmathbf%7BV%7D%5CrVert_F%5E2++%5Cright%29) 

参考：https://zhuanlan.zhihu.com/p/159087297

### 2.1.2 Deep Crossing模型--经典深度学习架构
1. 应用场景

   ![](https://pic2.zhimg.com/80/v2-d01272a3c3877126b9ab3bc7f135b901_720w.jpg) 

2. 网络结构 

   embedding层，stacking层，multiple residual units层，scoring层

​                           ![](https://pic3.zhimg.com/80/v2-31159f816eb0662eab198624dc75b356_720w.jpg) 

3. 反思

   embedding+多层神经网络，相较于传统的二阶特征交叉能力，deep crossing拥有深度交叉的能力 

### 2.1.2 NeuralCF - CF与深度学习的结合

先回忆一下传统的矩阵分解怎么做

1. 物品-用户共现矩阵分解成用户向量和物品向量

2. 向量embedding化

3. embedding后向量取内积（重要）

4. 得到分数

这个模型 复杂的位置就是在第三步操作上 使用多层神经网络去替换这个卷积操作

 ![](https://img-blog.csdnimg.cn/img_convert/a464defa6410390c728f22b494cb1aa5.png) 

==优势==

1. 利用神经网络来拟合任意函数，灵活地组成不同的特征，按需增加或减少模型的复杂度

==劣势==

1. 基于协同过滤构造,没有引入更多其他类型的特征

2. 在实践中，防止过拟合的风险

##  2.2 加强特征交叉能力

### 2.2.1 PNN模型

感谢：https://zhuanlan.zhihu.com/p/37522266

https://blog.csdn.net/wuzhongqiang/article/details/108985457

​             ![](https://img-blog.csdnimg.cn/20190516094615674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE4MjkzMjEz,size_16,color_FFFFFF,t_70) 

​	相较于 deep crossing模型中的stacking层，PNN模型替换成了乘积层。其他的在模型的输入，embeding层，多层神经网络以及最终的输出层上没有结构上的不同。

​	product layer层，左边为线性部分，认为 特征之间的关系是and“且”的一种关系，而非add"加"的关系。 

​										 *z*=*concat*([*e**m**b*1,*e**m**b*2..,*e**m**bn*],*axis*=1) 

   其右边操作为乘积操作，有内积和外积的区别。外积在操作上会将问题的复杂度从原来的m到 $m^2$,在选择上更应该慎重。

- 优势

​	定义了外积和内积操作更有针对性地强调不同特征之间的交互

- 局限

​    在外积操作上，为了效率经行大量的简化操作，对所有特征进行无差别的交叉，在一定程度上忽略了原始特征中包含的价值信息。

### 2.2.2 product layer 内积

 ![](https://pic4.zhimg.com/v2-c9c719cd18f73549b51c7ffbde5cb86b_b.jpg) 

 PNN中p的计算方式如下，即使用内积来代表pij： 

 ![](https://pic3.zhimg.com/v2-b35c16772a4c7d8cac4954b7542b34aa_b.jpg) 

### 2.2.3 product layer 外积

 ![](https://pic3.zhimg.com/v2-5d18b1de84ecd5ebc6c447e119207eda_b.jpg)

  OPNN中p的计算方式如下： 

 ![](https://pic2.zhimg.com/v2-e14839f73e05f917878e8dd13ee6fdbd_b.jpg) 

此时pij为M*M的矩阵，计算一个pij的时间复杂度为M*M，而p是N*N*M*M的矩阵，因此计算p的事件复杂度为N*N*M*M。从而计算lp的时间复杂度变为D1 * N*N*M*M。这个显然代价很高的。为了减少负责度，论文使用了叠加的思想，它重新定义了p矩阵：

 ![](https://pic4.zhimg.com/v2-2837d613d607efaf9b5154334d9a8367_b.jpg) 

## 2.3 记忆能力与泛化能力的综合

### 2.3.1 wide&deep模型

​	wide部分是让模型具有较强的“记忆能力”，deep部分是让模型具有泛化能力。这样的结构使模型兼具了逻辑回归和深度神经网络的优点--能快速处理并且记忆大量的历史行为特征，并且具有强大的表达能力。

 ![](https://img-blog.csdnimg.cn/20200302174556324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbnhpZA==,size_16,color_FFFFFF,t_70) 

在提出W&D模型，平衡Wide模型和Deep模型的记忆能力和泛化能力。实际上是lr+dnn。
记忆（memorization） 通过特征叉乘对原始特征做非线性变换，输入为高维度的稀疏向量。通过大量的特征叉乘产生特征相互作用的“记忆（Memorization）”，高效且可解释，但要泛化需要更多的特征工程。

泛化（generalization）只需要少量的特征工程，深度神经网络通过embedding的方法，使用低维稠密特征输入，可以更好地泛化训练样本中未出现过的特征组合。但当user-item交互矩阵稀疏且高阶时，容易出现“过泛化（over-generalize）”导致推荐的item相关性差

- 工程应用

 ![](https://pica.zhimg.com/v2-42250a1664527e7fbcd4e21f3a74230f_1440w.jpg?source=172ae18b) 

### 2.3.2 wide&deep进化 deep&cross模型

 ![](https://img-blog.csdnimg.cn/20190625111158977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1lhc2luMA==,size_16,color_FFFFFF,t_70) 

 Cross Network

​	交叉网络的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式：

 ![](https://img-blog.csdnimg.cn/20190625112122963.png) 	

​	xl和xl+1 分别是第l层和第l+1层cross layer的输出，wl和bl是这两层之间的连接参数。注意上式中所有的变量均是列向量，W也是列向量，并不是矩阵。xl+1 = f(xl, wl, bl) + xl. 每一层的输出，都是上一层的输出加上feature crossing f。而f就是在拟合该层输出和上一层输出的残差。

​	Cross Layer 设计的巧妙之处全部体现在上面的计算公式中，我们先看一些明显的细节：1) 每层的神经元个数都相同，都等于输入 的维度 

DCN能够有效地捕获有限度的有效特征的相互作用，学会高度非线性的相互作用，不需要人工特征工程或遍历搜索，并具有较低的计算成本。
1）提出了一种新的交叉网络，在每个层上明确地应用特征交叉，有效地学习有界度的预测交叉特征，并且不需要手工特征工程或穷举搜索。
2）跨网络简单而有效。通过设计，各层的多项式级数最高，并由层深度决定。网络由所有的交叉项组成，它们的系数各不相同。
3）跨网络内存高效，易于实现。
4）实验结果表明，交叉网络（DCN）在LogLoss上与DNN相比少了近一个量级的参数量

