title: 知识追踪综述

## 0 说点废话（讲一下怎么找到这个文章的）

2022年7月16日

在床上百无聊耐的看视频 突然看到这样的东西

[![](https://s1.ax1x.com/2022/07/17/jI9blT.png)](https://imgtu.com/i/jI9blT)

https://www.bilibili.com/video/BV1EY411K7wt

嗯嗯 我就去这个网站试了试

https://openknowledgemaps.org/

查了查 我留下的二课题 知识追踪（cool）

[![](https://s1.ax1x.com/2022/07/17/jI9j0J.md.png)](https://imgtu.com/i/jI9j0J)

发现了一篇 2022-4-1 号的 中文的一篇综述 就很开心

## 1. 前言-知识追踪是啥

​	KT算法将学生的知识掌握程度随着时间的推移建模预测，从而能够准确地预测学生在未来互动中的表现据此有针对性地为学生订制不同的学习路线，提升学习效率.学生通过在线学习平台进行学习交互，形成答题行为时间序列，KT算法通过对学习者和序列联合建模，预测其对于新知识的认知概率分布，进一步推理出学习者的技能和认知水平。

## 2. kt问题定义以及数据集

### 2.1 定义

[![](https://s1.ax1x.com/2022/07/17/jI9q6U.md.png)](https://imgtu.com/i/jI9q6U)

### 2.2 数据集

ASSISTments Data是KT领域最为经典的数据集，其中ASSISTments2009数据集 是绝大多数KT模型的标准数据集；EdNet［8］ 发布于2019年，是KT领域最新的数据集，提供了超过1亿条 学习者交互记录；Synathetic是 DKT模型所附带的数据集，包括了超过 20万条学习记录信息；其他数据集包括：Junyi15，algebra 2006 2007及Statics2011等.

| dataset           | Website                                                      | Field       | Records/k |
| ----------------- | ------------------------------------------------------------ | ----------- | --------- |
| ASSISTments2009   | https://sites.google.com/site/assistmentsdata/datasets       | Math        | 32        |
| ASSISTments2017   | https://sites.google.com/view/assistmentsdatamining/dataset  | Math        | 94        |
| Ednet             | https://github.com/riiid/ednet                               | English     | 13 000    |
| Junyi15           | https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=1275  | Math        | 2 500     |
| algebra 2006 2007 | https://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp        | Math        | 180       |
| Synathetic        | https://github.com/chrispiech/DeepKnowledgeTracing/tree/master/data/synthetic | Math        | 1 435     |
| Statics2011       | https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=507   | Engineering | 19        |

## KT过程分析

### 知识关系-- 使用图来表示知识关系

| 论文名                                                       | 方法                                                         | 效果                         | 年份 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------- | ---- |
| Graph ⁃ based knowledge tracing：modeling student proficiency using graph neural network | 构建知识图谱并进行KT 的方法，方法基于图结构                  | 提高了模型预测的可解释性     | 2019 |
| HGKT：introducing hierarchical exercise graph for knowledge tracing | 引入了问题模式的概念，构造了一个分层的练习图，可以对学习依赖关系进行建模，并采用两种注意机制 | 突出学习者的重要历史状态     | 2022 |
| Modeling relational data with graph convolutional networks   | 基于关系图卷积神经网络（CNN）的知识图谱构建方法              |                              | 2018 |
| Peer ⁃inspired student performance prediction in interactive online question pools with graph neural network | 在 R-GCN 的基础上，利用学生互动过程，构建了“学生—互动—问题”网络，提出了 R2GCN 模型 | 适用于异构情况下的网络学习   | 2020 |
| Machine learning and knowledge discovery in databases        | 提出了一种端到端的DKT框架，能够利用“高阶问题—技能”关系       | 缓解数据稀疏性和多知识点问题 | 2021 |

### 因素关系处理

DKT方面，由于问题与概念的不平等性，使得大多数KT方法都把模型中的知识点数量等价于问题数量，从而丢了问题所具有的个性化特性.如果缺乏对于问题的甄别，则无法确定问题的信度，进而损失模型的预测精度.

#### 项目反应理论（IRT）

项目反应理论（IRT）该理论假设学习者的学习能力不随时间和实践变化。（该理论认为 只有答会高水平的题目 才能证明学生是高水平的）

举个例子：

考生答对的题目难度是判断考生能力的标准。某考生答对10道难度为1的题目，获得的[能力值](https://www.zhihu.com/search?q=能力值&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"answer"%2C"sourceId"%3A"537050095"})依然是1，另一考生答对1道难度为8的题目，能力值则为8 

其中rasch模型 使用难度描述 输入模型的问题

[![jI9LXF.png](https://s1.ax1x.com/2022/07/17/jI9LXF.png)](https://imgtu.com/i/jI9LXF)

其中，*θ* 代表学习者的学习能力；*b* 代表问题的难度 .Rasch 模型在可解释性、问题区分性等方面性能优越，GHOSH等通过在深度模型中使用Rasch编码，提高了DKT的可解释性，取得了卓越的预测性能.

#### 深度技术下 IRT模型

| 模型名    | 论文名                                                       | 干啥                                                         | 年份 |
| --------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| Deep ⁃IRT | Deep ⁃ IRT：make deep learning based knowledge tracing explainable using item response theory | 它是 IRT 模型与 DKVMN 模型的结合 .                           | 2022 |
| DKVMN     | Dynamic key ⁃ value memory networks for knowledge tracing    | 在知识追踪中使用了内存机制                                   | 2017 |
| EKPT      | Learning or forgetting？A dynamic approach for tracking the knowledge proficiency of student | 提出知识熟练度追踪（KPT）模型和练习关联的知识熟练度（EKPT）模型，应用于知识估计、分数预 测和诊断结果可视化三个重要任务. | 2020 |
| KTMs      | Knowledge tracing machines：factorization machines for knowledge tracing | 综合IRT，AFM，PFA等模型，提出了知识追踪机（KTMs）框架，KTMs利用所有特征的稀疏权值集，对学习者答题结果的概率进行建模. | 2019 |

### 学习认知机制和遗忘机制

#### 认知机制

WANG等［27］提出了一种通用的神经认知诊断框架，摒弃人工特征，将神经网络集成到复杂的非线性交互模型中，解决认知诊断问题，并且结合CNN，提出了Neural CDM+模型，通过自动提取系统中的知识点信息，补充知识点相关度矩阵，避免了主观性甚至错误.

#### 遗忘机制

DKT模型使用RNN一定程度上实现了对记忆过程模拟，但是仍然没有真正意义上模拟人类思维习惯.

| 模型  | 论文名                                                    | 方法                                                         | 年份 |
| ----- | --------------------------------------------------------- | ------------------------------------------------------------ | ---- |
| LPKT  | Learning Process-consistent Knowledge Tracing             | 加了个遗忘层 主要是sigmoid                                   | 2021 |
| DKVMN | Dynamic key ⁃ value memory networks for knowledge tracing | 过类似于计算机内存管理的方式，建立知识记忆遗忘矩阵，在模型可解释性上取得了很大的进步 | 2017 |
| CKT   | Context-aware attentive knowledge tracing                 | 基于Transformer的模型框架 上引入了注意力衰减机制，模拟全局遗忘行为，从而取得了较好的模型效果 | 2020 |

## KT方法

### 传统kt方法

[![](https://s1.ax1x.com/2022/07/17/jI9HpV.png)](https://imgtu.com/i/jI9HpV)

#### BKT

特点

1. 标准BKT模型建模过程中将知识点设置为“永不忘记”，并且假设一个题目只对应一个知识点（按道理来说 一个题目是对应多个知识点的）

2. 在其中有几种概率 

   2.1 *P* ( *L*)是初始知识状态下学生掌握相关知识点的概率

   2.2 *P* (*T* )为经过练习后学生掌握目标知识点的概率

   2.3 *P* (*G*)表示学生猜对答案的概率（有趣）

   2.4 *P* ( *S*)为学生掌握知识点但做错题目的概率

   [![](https://s1.ax1x.com/2022/07/17/jI9Xm4.md.png)](https://imgtu.com/i/jI9Xm4)



### DKT

#### 基于RNN的KT

整体来讲，基于RNN结构的追踪模型在性能和可用性方面大幅度超越了传统模型，但是在解释性上略显不足.

| 模型    | 论文                                                         | 方法                                                         | 年份 |
| ------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| DKT-DSC | Deep knowledge tracing and dynamic student classification for knowledge tracing | 通过在每个时间间隔内将学生分组，预测学生的学习效果           | 2018 |
| .。。   | Incorporating features learned by an enhanced deep knowledge tracing model for stem/non-stem job prediction | 采用DKT进行知识状态预测，证明了DKT模型在实际工作中的有效性.  | 2019 |
| EERNN   | Exercise⁃enhanced sequential modeling for student performance prediction | 通过追踪学生的练习记录和相应练习的文本内容，提出了一个通用的练习增强循环神经网络（EERNN）框架 | 2018 |

#### 基于注意力的DKT

通过注意力机制，可以在过去的交互序列中寻找到与当前问题相关的重信息，从而做出更为准确的预测，并且证明了基于 Transformer的模型比基于 RNN的模型在运算速度上快了一个数量级.

| 模型            | 论文                                                         | 效果                                                         | 年份 |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| transformer     | A self ⁃ attentive model for knowledge tracing               | 必然伴随着对过去相关练习交互的回忆                           | 2019 |
| 双向transformer | Towards an appropriate query，key，and value computation for knowledge tracing | 将练习序列和回答序列分别进行编码，从而寻找到了更为合适的 Query | 2020 |
| Saint+          | Saint+：integrating temporal features for EdNet correctness prediction | 将经过时间、滞后时间两个特征编码与学生答题响应的编码进行结合，从而增强了 模型的预测精度. | 2021 |

#### 基于hawkes过程的DKT

https://blog.csdn.net/guanlily123/article/details/100145889

.Hawkes过程则假设过去事件会在一定程度上提高未来事件发生的概率，并且这种影响会随着时间指数衰减，这种思想比较符合认知遗忘规律下的学习者能力.

| 模型           | 论文                                                         | 效果                                                         | 年份 |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| LSTM           | The neural Hawkes process：a neurally self⁃modulating multivariate point process | 利用Hawkes过程对长短期记忆（LSTM）节点的时间效应（遗忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假定事件相互独立，并不符合多知识点状态下学习者交互的逻辑 | 2017 |
| Hawkes Process | Temporal cross⁃effects in knowledge tracing                  | 定事件相互独立，并不符合多知识点状态下学习者交互的逻辑忘效应）进行衰减处理.KT领域的学习者交互过程可以被看作是一系列的连续事件流，但是泊松过程假 深入研究了不同知识点之间的时间交叉效应，并且提高了深度模型的可解释性 | 2021 |

## 展望

本文作者对比讨论了目前主流的KT模型，分析了主流模型的优缺点.目前的研究主要针对知识点与题目间的关系进行建模，很少有研究从模型效果评价指标、学习潜力预测、深度记忆过程模拟等方面进行知识状态追踪和预测，同时也较少有对多知识点关系建模方法进行知识状态追踪的研究.通过分析KT领域目前主流的模型，梳理出KT领域未来的发展方向，从数据表征、认知建模、建模方法、解释及反馈方面对KT领域进行展望. 

1）数据处理及数据表征.KT模型在运用输入数据方面越来越需要预处理、预训练操作.预训练模型在序列任务上表现出了良好的性能，采用可解释性较强的算法预处理输入数据变得越来越重要.比如使用Rasch编码预处理输入数据后，再进行注意力运算和模型预测，在模型性能和可解释性方面都取得了很好的效果 .在数据特征方面，引入学习者生物特征、更加丰富的习题特征都是未来重要的突破方向，KT模型应该向更高维度、更普适、更泛化的方向发展，如何对学习者的非结构性学习数据进行追踪也是重要的发展方向. 

2）认知建模 .认知诊断和 KT分别应用于学习者静态数据分析和动态数据分析，但 KT模型内不应缺乏对学习者认知能力的建模.对于问题维度、知识点维度的建模不足以拟合学习者的知识状态变化，应在此基础上进一步对认知维度进行建模，从而在更高的维度上追踪学习者的状态变化情况. 

3）模型方法及可解释性 .自从 DKT被提出以来，KT领域内的模型基本以深度模型为主，但越来越多的工作表明 DKT无法做到真正的动态自适应 KT.基于 RNN 的模型在数据拟合能力上逐步被以注意 力机制为核心的Transformer类模型超越，未来KT领域建模方法应该在注意力方向、图谱方向进一步发展.人脑记忆的形成过程中，人自身的注意力是重要的一环，这也是基于注意力机制模型结合遗忘建模取得不错效果的关键原因.知识图谱作为非结构化知识表征的重要手段，在KT领域有更进一步的潜力， 并且对于认知能力研究也可以加入图谱技术，从而在可解释性KT方向取得突破.