---
title: ctr 综述
---

## 说些废话

点击率预估(CTR)在各种个性化在线服务中扮演着重要的角色，包括：计算广告、推荐系统和Web搜索等。 

根据时间来分 可以稍微看看这个图！

2007年 以前 都是 LR+人工特征工程为主 

到了2010年 以 隐向量为代表的FM 模型，解决了人工组合特征的困扰

而在2014 年 facebook提出的 GBDT + LR  提出的一种树模型特点构建组合特征的思路

2015年以后，借助非线性自动组合特征能力的深度模型，开始成为业界的主流

从经典的DNN 到 结合浅层的Wide&Deep 用于CTR预估模型的深度模型在这些年百花盛开。

各种交叉特征建模方法层出不穷，attention机制也帮助更好的适应业务，提升模型的解释性。

而核心问题离不开解决数据高维稀疏难题，自动化组合特征，模型可解释性等问题。

## 主要看了什么





## 1. 前言 -- CTR推荐是啥

### ctr是啥

CTR指在搜索引擎中输入关键词后进行搜索，然后按竞价等因素把相关的网页按顺序进行排列出来，然后用户会选择自己感兴趣的网站点击进去；把一个网站所有搜索出来的次数作为总次数，把用户点击并进入网站的次数占总次数的比例叫点击率。

计算公式

计算公式为CTR=实际点击次数/展示量，即 Click / Show content。

CTR：点击通过率，Click-Through-Rate (点击通过比率)

### ctr预估推荐

[![](https://s1.ax1x.com/2022/09/24/xA56df.md.png)](https://imgse.com/i/xA56df)

预测点击率(CTR）的算法被称为CTR预估算法，CTR预估的一系列算法经常用于推荐系统的排序阶段，其实有很多机器学习的方法或者深度学习的方法都可以作为CTR预估的算法，前提是这些算法的输出是一个0~1的概率值，并以概率作为分类的依据。

## 2. 问题定义以及评价

ctr推荐并不相同与ctr预估。其目标存在差距。

CTR预估起源于计算广告，因为关系到真金白银的定价问题，因此要求预估出来的CTR必须“**绝对准确**”。这是因为，假如给一个用户准备了A/B/C三个广告，那么无论预测CTR是0.9、0.8、0.6，还是0.5、0.4、0.3都不影响三个广告的展现顺序，但是向客户的收费却有天壤之别。

但是推荐系统只要求“**相对准确**”。假如ABC换成了三篇文章，只要能够将用户最喜欢的A排在第1位，次喜欢的B排在第2位，无论我们预测的CTR是0.9、0.8、0.6，还是0.5、0.4、0.3，用户都能接受。

### 2.1 定义

其输入是用户的点击和未点击的数据。通过分析，输出一个相对排序的item 进行推荐

### 2.2 数据集

数据集以点击数据集为主

同样的 我看到的数据集类型基本都是广告这个类型

一定比的数据集有Criteo，azavu

其他的有 kdd12，ml-1m，iPinYou等

| dataset | website                                                   | filed    | records/k |
| ------- | --------------------------------------------------------- | -------- | --------- |
| Criteo  | https://www.kaggle.com/c/criteo-display-ad-challenge/data | 广告点击 | 45,850    |
| azavu   | https://www.kaggle.com/c/avazu-ctr-prediction/data        | 广告点击 | 40,428    |
| iPinYou | http://contest.ipinyou.com/                               | 广告点击 | 15,367    |
| kdd2010 | https://grouplens.org/datasets/mov                        | 教育评分 |           |
| ml-1m   | https://grouplens.org/datasets/movielens/                 | 电影评分 | 1000      |

### 2.3 常用评估指标

一定会比较的 AUC logloss

其他的 RelaImpr Rmse RIG

- AUC

https://blog.csdn.net/fanfangyu/article/details/122885441

是一个模型的评价指标，只能适用于二分类模型的评价。

[![](https://s1.ax1x.com/2022/09/24/xA5RJg.jpg)](https://imgse.com/i/xA5RJg)

AUC（Area Under Curve）被定义为ROC曲线下与坐标轴围成的面积，

其中，ROC曲线全称为受试者工作特征曲线（receiver operating characteristic curve）

二元分类模型的预测结果有四个结局。即以下四种。

ROC曲线的横坐标是**伪阳性率**（也叫假正类率，False Positive Rate），纵坐标是**真阳性率**（真正类率，True Positive Rate），相应的还有**真阴性率**（真负类率，True Negative Rate）和**伪阴性率**（假负类率，False Negative Rate）。这四类指标的计算方法如下： 

[![](https://s1.ax1x.com/2022/09/24/xA5co8.png)](https://imgse.com/i/xA5co8)

其中坐标轴的定义：

（1）伪阳性率（[FPR](https://baike.baidu.com/item/FPR/6343296?fromModule=lemma_inlink)）：判定为正例却不是真正例的概率，即真负例中判为正例的概率

（2）真阳性率（[TPR](https://baike.baidu.com/item/TPR/5548598?fromModule=lemma_inlink)）：判定为正例也是真正例的概率，即真正例中判为正例的概率（也即正例召回率）

（3）伪阴性率（[FNR](https://baike.baidu.com/item/FNR/5609400?fromModule=lemma_inlink)）：判定为负例却不是真负例的概率，即真正例中判为负例的概率。

（4）真阴性率（[TNR](https://baike.baidu.com/item/TNR/4663071?fromModule=lemma_inlink)）：判定为负例也是真负例的概率，即真负例中判为负例的概率。

关于曲线的绘制 得看这个

https://blog.csdn.net/xiaohuihui1994/article/details/87987836

步骤大致分为三步：

1. 按照属于“正样本”的概率将所有样本排序
2. 将分类阈值设为最大，即把所有样例均预测为反例，所以得到此时的p和r均为0
3. 依次将样本从高到低的样本score值作为阈值

- logloss

二分类时 logloss 就是交叉熵

- RelaImpr

$$
Relalmpr = (\frac {AUC(meadured\quad model)-0.5}{AUC(base\quad model)-0.5}-1)\times 100\%
$$

RelaImpr代表相对于based model的相对改进指标，对于随机猜测的话，值AUC是0.5 （阿里DIN）

- RMSE

均方根误差

$$
RMSE = \sqrt\frac{\sum_{i=1}^{N}(Predicted_i - Actual_i)^{2}}{N}
$$
均方根误差RMSE和标准差的计算公式也是高度近似的：标准差是用来衡量一组数自身的离散程度，而均方根误差是用来衡量观测值（真值）与预测值之间的偏差

- RIG

相对信息增益(Relative Information Gain) 

信息熵公式为：
$$
h(X) = - \sum_{i=1}^{n} p_i\log p_i
$$
在条件A一直的情况下，X的相对熵为：
$$
h(X|A) = \sum_{i=1}^{n} p_i(x_i|A)\log p_i(x_i|A)
$$
熵与条件熵的差值为信息增益：
$$
g(X,A) = H(X) - H(X|A)
$$
信息增益值的大小是相对数据集而言的，并没有绝对的意义，在分类问题比较困难的时候，也就是训练数据集的经验熵比较大的时候，信息增益的值比较大，反之，信息增益值会偏小。使用相对信息增益(信息增益比)可以对这一个问题进行校正，相对信息增益(Relative Information Gain):
$$
RIG =\frac {H(X) - H(X|A)}{H(X)}
$$
RIG指标不仅和模型的质量有关，还和数据集的分布情况有关；因此千万注意不可以使用RIG来对比不同数据集上生成的模型，但可以用来对比相同数据集上不同模型的质量差异。 

### 2.4 模型大类

| 模型大类  | 提出时间   | 原理                                                         | 优势                                               | 局限性                                                       |
| --------- | ---------- | ------------------------------------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ |
| LR        | 2010年以前 | 使用线性权重组合每个单一的特征                               | 具有效率高、易于快速部署、快速挖掘有效特征的优点   | 不具备特征交互的能力，交叉特征可能比单一特征具有更重要的影响 |
| FM        | 2010       | 对Poly2中的权重矩阵W做矩阵分解，为每个特征学一个k为的向量表示。两个向量的内积表示特征对的重要性 | FM可以捕获特征交互，同时可以在稀疏场景下有效的学习 | FM忽略了这样一个事实: 当一个特性与来自其他域(Field)的特性交互时，它的行为可能会有所不同。 |
| DNN       | 2015       | 利用深度网络，提高模型的交叉特征建模能力                     | 偏重于挖掘高阶，增强了模型的表达能力               | 对低阶交叉特征信息利用不足，可解释性差                       |
| wide&deep | 2016       | 通过联合训练的方式，同时获得线性模型（Wide）的记忆能力和深度模型（Deep）的泛化能力，提高模型的预测准确性和多样性 | 具有线性和深度两个特性                             | 模型复杂 训练时间长                                          |

## 3. 模型

### 3.1 LR模型

- LR

LR( Logistic Regression ) 是机器学习中一种线性分类模型， 由于其简单、高效、易并行的特点，在实际生产环境中被广泛地应用。 

同样的在实际广告场景下的广告点击率预估问题。对于 给定年龄、性别、教育程度、兴趣类目等用户侧特征，上下文特征，以及广告id，商品类型等广告侧特征。预测用户对于特征的点击概率。针对特征的做法是对各类特征进行one-hot编码，针对连续数值特征，会按照区间分段的方法将其离散化再进行one-hot编码。

对于LR的模型来说，需要提供成熟的特征工程工作，而对应的模型结构较为简单。对于其模型来说没有使用到组合特征，是一大问题。
$$
\hat{y}(x) = w_o + \sum_{i=1}^{n} w_ix_i
$$


- poly2

模型有效的解决了LR的组合特征的问题，需要o（m2）的参数空间。 特征xi和xj的两两组合用xixj表示，可见只有当xi和xj都为非零值时，组合特征（或称交叉项）在多项式模型中才有意义。在真实业务场景下，样本特征通常高维稀疏，而组合特征xixj的稀疏程度则更为严重。在这种极度稀疏的情形下，模型很难准确地学习出参数wij 。 因为特征数据十分稀疏，模型在训练样本中从未见过有效的交叉项xixj，无法学习这个交叉项和label之间的关系。而参数无法准确学习，必然会严重影响模型效果。 
$$
\hat{y}_{Poly2}(x) = w_0 + \sum_{i=1}^{n} w_ix_i +\sum_{i=1}^{n}\sum_{j=i+1}^{n}w_{ij}x_ix_j
$$
  

| 模型名 | 论文名 | 优势                                                   | 缺陷                                                         | 年份 |
| ------ | ------ | ------------------------------------------------------ | ------------------------------------------------------------ | ---- |
| LR     |        | 简单、高效、易并行的特点，在实际生产环境中被广泛地应用 | 缺少组合特征                                                 | 2007 |
| poly2  |        | 相较于LR 添加了 组合特征                               | 稀疏特征向量会变得更加稀疏，无法收敛。权重参数由n直接上升到 n2，增加了训练复杂度 | 2010 |

