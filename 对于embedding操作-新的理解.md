---
title: 对于embedding操作 新的理解
---

2022年5月1日21点20分

 ![img](https://img1.baidu.com/it/u=3901286021,700704377&fm=253&fmt=auto&app=120&f=GIF?w=320&h=180) 

先放一张图片 high！ 一下 

在这里终于能理解到 为什么pytorch 的好用 

和我对embedding 操作 将近一个月的误解！！

我真的是high到不行了

### 0. 前言 （很重要）

​	对于一位5月1号 在看着风骚律师 在看着一个名字叫做caser推荐论文的同学。这里面论文里面的一句话，深刻的激发了我。即 俺终于知道如何编码序列推荐数据了！

​	其实 在这里面最主要的一个体会是！

​	勇敢 即 主动迈出那一步！！ 

​	说太多了

​	先看看论文里面的那句话！

[![OC6pB4.md.png](https://s1.ax1x.com/2022/05/01/OC6pB4.md.png)](https://imgtu.com/i/OC6pB4)

​	这里的set和universe 用的太美了 brilliant！ 

​	下面这句话非常重要 

​	**用户的交互序列其实就是一个物品序列的组合！！！**

## 1. 自己以前的误解

### 1.1 误解1

本人一直以为 embedding 一定得是one-hot 转换成 embedding-vector

即 我认为简单数字是不能embedding的

大错特错！！！

数字为什么就不能表示物品的特征呢！

数字为什么就不能表示物品的特征呢！

数字为什么就不能表示物品的特征呢！

[![OCcQLF.png](https://s1.ax1x.com/2022/05/01/OCcQLF.png)](https://imgtu.com/i/OCcQLF)

数字 简单，那么可爱  就应该被embedding 

我觉得我这里的误解 应该是犯了教条主义的错误！！！

哈哈哈

### 1.2 误解2

即 我认为的交互序列是存在点击和不点击这样子的其他属性的

正好最近阅读了 知识追踪 反而没起到正作用 反而放这个让我的误解加深了！

论文里面的一句话！

用户序列 中的元素 即为物品序列的子元素

[![OCgGnS.md.png](https://s1.ax1x.com/2022/05/01/OCgGnS.md.png)](https://imgtu.com/i/OCgGnS)

### 1.3 误解3

我觉得这里 应该是犯了 实践-理论-实践 这个基本道路的错误

为什么 你在没有实践后 就贸然的翻阅理论呢！

可笑!!

我这一周 基本一直在思考 如何找人家 是如何讲数据集 处理成 序列推荐 模型的数据

然后 一直在抱怨 为什么 我的数据集 相对 cv 的 数据 是多难处理

最后 在读caser 这篇文章的时候 才发现自己是多么的沙雕！

本来就是一句话的事情 自己想的太复杂了

​                                            <img src="https://img0.baidu.com/it/u=1720648519,1474192396&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=300&amp;h=300" alt="img" style="zoom:50%;" /> 

- 心得1

多看论文 说不定有个人能说的很明白 一句话的事儿

- 心得2

多实践 只有多试试 才能 发现找到新的道路

## 2. 上代码

```python
# 老家伙
import torch
import torch.nn as nn

# 如果 前一个是物品的个数 后面是物品embedding 维度
# 有趣的事情发生了
# 在文章中 咱只需要 确认一个用户编码 还有一个物品编码就好了嘛
# 你不知道 我在写上面的话是多高兴
embedding = nn.Embedding(10, 3)
```

```python
input = torch.LongTensor([1,2,3,4,5,6,7,8,9])
input_embed = embedding(input)
input_embed
# output
'''
tensor([[ 0.3974, -0.0988,  0.3758],
        [-0.0685, -0.5384, -0.4036],
        [-0.1967,  1.4902, -1.7089],
        [ 1.6544, -0.3214,  0.1649],
        [ 0.6180,  0.9051,  0.7922],
        [ 0.0805,  0.9490, -0.6889],
        [ 1.4841,  0.8647,  0.0356],
        [ 0.2589,  1.4257, -0.8124],
        [ 0.6450,  0.5506,  0.4622]], grad_fn=<EmbeddingBackward0>)
'''

```

```python
## 真的有趣的部分
input_embed = embedding(torch.LongTensor([1,2,3,8]))
input_embed
# output
'''
tensor([[ 0.3974, -0.0988,  0.3758],
        [-0.0685, -0.5384, -0.4036],
        [-0.1967,  1.4902, -1.7089],
        [ 0.2589,  1.4257, -0.8124]], grad_fn=<EmbeddingBackward0>)
'''
```

观察两个output 

其 元素是对应的

其元素是对应的

对应的

好的 我说完了！