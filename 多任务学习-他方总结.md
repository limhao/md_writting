---
title: 多任务学习 他方总结
---

2022/11/8

链接：https://zhuanlan.zhihu.com/p/363059498 一文梳理多任务学习(MMoE/PLE/DUPN/ESSM等)



## 多任务为什么有效

- **隐式数据增强**：每个任务都有自己的样本，使用多任务学习的话，模型的样本量会提升很多。而且数据都会有噪声，如果单学A任务，模型会把A数据的噪声也学进去，如果是多任务学习，模型因为要求B任务也要学习好，就会忽视掉A任务的噪声，同理，模型学A的时候也会忽视掉B任务的噪声，因此多任务学习可以学到一个更精确的嵌入表达。
- **注意力聚焦**：如果任务的数据噪声非常多，数据很少且非常高维，模型对相关特征和非相关特征就无法区分。多任务学习可以帮助模型聚焦到有用的特征上，因为不同任务都会反应特征与任务的相关性。
- **特征信息窃取**：有些特征在任务B中容易学习，在任务A中较难学习，主要原因是任务A与这些特征的交互更为复杂，且对于任务A来说其他特征可能会阻碍部分特征的学习，因此通过多任务学习，模型可以高效的学习每一个重要的特征。
- **表达偏差**：MTL使模型学到所有任务都偏好的向量表示。这也将有助于该模型推广到未来的新任务，因为假设空间对于足够多的训练任务表现良好，对于学习新任务也表现良好。
- **正则化**：对于一个任务而言，其他任务的学习都会对该任务有正则化效果。



## DUPN

![](https://pic1.imgdb.cn/item/636b0ece16f2c2beb185266e.png)

 模型分为行为序列层、Embedding层、LSTM层、Attention层、下游多任务层(CTR、LTR、时尚达人关注预估、用户购买力度量)。如下图所示 

## （图b为OMoE）

![](https://pic1.imgdb.cn/item/636b0ece16f2c2beb18526d0.png)

如下图所示，模型(a)最常见，共享了底层网络，上面分别接不同任务的全连接层。模型(b)认为不同的专家可以从相同的输入中提取出不同的特征，由一个Gate(类似) attention结构，把专家提取出的特征筛选出各个task最相关的特征，最后分别接不同任务的全连接层。MMOE的思想就是对于不同任务，需要不同专家提取出的信息，因此每个任务都需要一个独立的gate。 

##  **PLE** 

 即使通过MMoE这种方式减轻负迁移现象，跷跷板现象仍然是广泛存在的(跷跷板现象指多任务之间相关性不强时，信息共享就会影响模型效果，会出现一个任务泛化性变强，另一个变弱的现象）。PLE的本质是MMOE的改进版本，有些expert是任务专属，有些expert是共享的，如下图CGC架构，对于任务A而言，通过A的gate把A的expert和共享的expert进行融合，去学习A。 

![](https://pic1.imgdb.cn/item/636b0ece16f2c2beb1852676.png)

真实的模样！

![](https://pic1.imgdb.cn/item/636b0ece16f2c2beb185266b.png)

## AITM

业务模型：信用卡有5个状态  五个状态（impression->click->application->approval->activation），链路很长。样本非常不均衡，即正样本越到后面越少，因此，前面任务的信息如何传递到后面任务，是本文研究的一个话题。

论文提出了一个**AIT（Adaptive Information Transfer）**模块进行信息的传递，此外，在任务与任务之间，论文还增加了一个辅助损失函数 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af800.jpg)

其中 qt 是当前任务塔的输出， zt−1 是前面一个任务塔t-1的AIT模块的输出。所以变量z地位可以理解为循环式神经网络隐藏层的地位。 gt−1 是一个函数（网络层），他的物理意义是决定两个任务之间什么信息可以流动。 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af7f8.jpg)

损失函数 （生产环境下！）

![](https://pic1.imgdb.cn/item/636b1db316f2c2beb19d9dbc.png)

如果yt-1≥yt， Llc(θ)将输出一个正惩罚项，否则输出0。



## MetaHeac

**MetaHeac**摒弃底部bottom共享，tower分离的模式，认为**tower层的信息也是可以共享的**，比如有的item不会被点击那么也不会被转化，MetaHeac采用Gate机制同时控制bottom（Expert）和tower（Critic），并且采用**元学习（meta learning）**来学到**任务通用的参数**，再通过Gate机制选择任务特异的特征和预测结果的组合。 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af7fb.jpg)

