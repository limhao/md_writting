---
title: 多任务学习 他方总结
---

2022/11/8

链接：https://zhuanlan.zhihu.com/p/363059498 一文梳理多任务学习(MMoE/PLE/DUPN/ESSM等)

链接：https://www.bilibili.com/video/BV1354y1Z7jD 这个老师讲的也很好的 

# 多任务为什么有效

- **隐式数据增强**：每个任务都有自己的样本，使用多任务学习的话，模型的样本量会提升很多。而且数据都会有噪声，如果单学A任务，模型会把A数据的噪声也学进去，如果是多任务学习，模型因为要求B任务也要学习好，就会忽视掉A任务的噪声，同理，模型学A的时候也会忽视掉B任务的噪声，因此多任务学习可以学到一个更精确的嵌入表达。
- **注意力聚焦**：如果任务的数据噪声非常多，数据很少且非常高维，模型对相关特征和非相关特征就无法区分。多任务学习可以帮助模型聚焦到有用的特征上，因为不同任务都会反应特征与任务的相关性。
- **特征信息窃取**：有些特征在任务B中容易学习，在任务A中较难学习，主要原因是任务A与这些特征的交互更为复杂，且对于任务A来说其他特征可能会阻碍部分特征的学习，因此通过多任务学习，模型可以高效的学习每一个重要的特征。
- **表达偏差**：MTL使模型学到所有任务都偏好的向量表示。这也将有助于该模型推广到未来的新任务，因为假设空间对于足够多的训练任务表现良好，对于学习新任务也表现良好。
- **正则化**：对于一个任务而言，其他任务的学习都会对该任务有正则化效果。

# 任务关系的隐式建模

## shared-bottom

![](https://pic1.imgdb.cn/item/636db58d16f2c2beb158f22c.png)

一般用于 两个任务适不适合做多任务！ 做一个验证

一般来说 这个模型会出现 负迁移现象

## MMoE（图b为OMoE）

![](https://pic1.imgdb.cn/item/636b0ece16f2c2beb18526d0.png)

如下图所示，模型(a)最常见，共享了底层网络，上面分别接不同任务的全连接层。模型(b)认为不同的专家可以从相同的输入中提取出不同的特征，由一个Gate(类似) attention结构，把专家提取出的特征筛选出各个task最相关的特征，最后分别接不同任务的全连接层。MMOE的思想就是对于不同任务，需要不同专家提取出的信息，因此每个任务都需要一个独立的gate。 

![](https://pic1.imgdb.cn/item/636db58d16f2c2beb158f221.png)

mmoe 在使用上会存在**跷跷板**效应（即 一个效果提升 一个效果下降）

##  **PLE** 

即使通过MMoE这种方式减轻负迁移现象，**跷跷板现象**仍然是广泛存在的(跷跷板现象指多任务之间相关性不强时，信息共享就会影响模型效果，会出现一个任务泛化性变强，另一个变弱的现象）。PLE的本质是MMOE的改进版本，有些expert是任务专属，有些expert是共享的，如下图CGC架构，对于任务A而言，通过A的gate把A的expert和共享的expert进行融合，去学习A。 

![](https://pic1.imgdb.cn/item/636db58d16f2c2beb158f216.png)

## 总结

![](https://pic1.imgdb.cn/item/636db58d16f2c2beb158f20d.png)

# 任务关系的显示建模

## ESSM

![](https://pic1.imgdb.cn/item/636db5fd16f2c2beb1598957.png)

解决的问题：

1. cvr 正样本数量稀疏
2. cvr 模型训练样本空间和线上测试样本空间。不符合独立同分布。

how：

1. cvr 与ctr 的embedding共享，使得cvr网络可以在未点击曝光样本中学习
2. 通过引入ctcvr任务，将ctcvr与ctr任务定义在全样本空间，来辅组cvr在全样本空间建模

## ESSM2

更长的序列

![](https://pic1.imgdb.cn/item/636db5fd16f2c2beb1598946.png)

## AITM

业务模型：信用卡有5个状态  五个状态（impression->click->application->approval->activation），链路很长。样本非常不均衡，即正样本越到后面越少，因此，前面任务的信息如何传递到后面任务，是本文研究的一个话题。

塔之间的传递 不在只是使用乘法！

论文提出了一个**AIT（Adaptive Information Transfer）**模块进行信息的传递，此外，在任务与任务之间，论文还增加了一个辅助损失函数 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af800.jpg)

其中 qt 是当前任务塔的输出， zt−1 是前面一个任务塔t-1的AIT模块的输出。所以变量z地位可以理解为循环式神经网络隐藏层的地位。 gt−1 是一个函数（网络层），他的物理意义是决定两个任务之间什么信息可以流动。 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af7f8.jpg)

损失函数 （生产环境下！）

![](https://pic1.imgdb.cn/item/636b1db316f2c2beb19d9dbc.png)

如果yt-1≥yt， Llc(θ)将输出一个正惩罚项，否则输出0。

图！

## MetaHeac

**MetaHeac**摒弃底部bottom共享，tower分离的模式，认为**tower层的信息也是可以共享的**，比如有的item不会被点击那么也不会被转化，MetaHeac采用Gate机制同时控制bottom（Expert）和tower（Critic），并且采用**元学习（meta learning）**来学到**任务通用的参数**，再通过Gate机制选择任务特异的特征和预测结果的组合。 

![](https://pic1.imgdb.cn/item/636b1bca16f2c2beb19af7fb.jpg)

# 多loss优化策略

优化策略 主要是 在不同任务之间 提供单任务的loss权重 

即 
$$
L(t)= \sum {}_{i}w_{i}(t)L_{i}(t)
$$
确认 Loss 前面权重t是啥样子

## GradNorm

![](https://pic1.imgdb.cn/item/636db5fd16f2c2beb1598934.png)

## UWL

![](https://pic1.imgdb.cn/item/636db5fd16f2c2beb159892a.png)



