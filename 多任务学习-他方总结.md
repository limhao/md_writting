---
title: 多任务学习 他方总结
---

2022/11/8

链接：https://zhuanlan.zhihu.com/p/363059498 一文梳理多任务学习(MMoE/PLE/DUPN/ESSM等)



## 多任务为什么有效

- **隐式数据增强**：每个任务都有自己的样本，使用多任务学习的话，模型的样本量会提升很多。而且数据都会有噪声，如果单学A任务，模型会把A数据的噪声也学进去，如果是多任务学习，模型因为要求B任务也要学习好，就会忽视掉A任务的噪声，同理，模型学A的时候也会忽视掉B任务的噪声，因此多任务学习可以学到一个更精确的嵌入表达。
- **注意力聚焦**：如果任务的数据噪声非常多，数据很少且非常高维，模型对相关特征和非相关特征就无法区分。多任务学习可以帮助模型聚焦到有用的特征上，因为不同任务都会反应特征与任务的相关性。
- **特征信息窃取**：有些特征在任务B中容易学习，在任务A中较难学习，主要原因是任务A与这些特征的交互更为复杂，且对于任务A来说其他特征可能会阻碍部分特征的学习，因此通过多任务学习，模型可以高效的学习每一个重要的特征。
- **表达偏差**：MTL使模型学到所有任务都偏好的向量表示。这也将有助于该模型推广到未来的新任务，因为假设空间对于足够多的训练任务表现良好，对于学习新任务也表现良好。
- **正则化**：对于一个任务而言，其他任务的学习都会对该任务有正则化效果。



## DUPN

[![v2-520db381c459b615c30e7841b7401949-720w.webp](https://i.postimg.cc/bvTDQm4g/v2-520db381c459b615c30e7841b7401949-720w.webp)](https://postimg.cc/VSJkwWf0)

 模型分为行为序列层、Embedding层、LSTM层、Attention层、下游多任务层(CTR、LTR、时尚达人关注预估、用户购买力度量)。如下图所示 

## MMoE

[![v2-2724353e8849e0bdd26ef4e5c2f1d541-720w.webp](https://i.postimg.cc/5t1QYhKj/v2-2724353e8849e0bdd26ef4e5c2f1d541-720w.webp)](https://postimg.cc/k21gLpmd)

如下图所示，模型(a)最常见，共享了底层网络，上面分别接不同任务的全连接层。模型(b)认为不同的专家可以从相同的输入中提取出不同的特征，由一个Gate(类似) attention结构，把专家提取出的特征筛选出各个task最相关的特征，最后分别接不同任务的全连接层。MMOE的思想就是对于不同任务，需要不同专家提取出的信息，因此每个任务都需要一个独立的gate。 

##  **PLE** 

 即使通过MMoE这种方式减轻负迁移现象，跷跷板现象仍然是广泛存在的(跷跷板现象指多任务之间相关性不强时，信息共享就会影响模型效果，会出现一个任务泛化性变强，另一个变弱的现象）。PLE的本质是MMOE的改进版本，有些expert是任务专属，有些expert是共享的，如下图CGC架构，对于任务A而言，通过A的gate把A的expert和共享的expert进行融合，去学习A。 

[![v2-783e2e10da94bc581eeabcf575ab5a06-720w.webp](https://i.postimg.cc/pLKndWZg/v2-783e2e10da94bc581eeabcf575ab5a06-720w.webp)](https://postimg.cc/HrsLZCF2)

真实的模样！

[![v2-6f0786458500b49caf0d077a5b1e2c27-720w.webp](https://i.postimg.cc/jdp7KwzH/v2-6f0786458500b49caf0d077a5b1e2c27-720w.webp)](https://postimg.cc/GHj2xpgt)

